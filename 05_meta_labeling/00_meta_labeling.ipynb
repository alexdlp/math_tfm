{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Labeling\n",
    "\n",
    "Meta-labeling is particularly helpful when you want to achieve higher F1-scores. First, we build a model that achieves high recall, even if the precision is not particularly high. Second, we correct for the low precision by applying meta-labeling to the positives predicted by the primary model.\n",
    "\n",
    "The central idea is to create a secondary ML model that learns how to use the primary model. This leads to improved performance metrics, including: Accuracy, Precision, Recall, and F1-Score etc.\n",
    "\n",
    "Binary classification problems present a trade-off between type-I errors (false positives) and type-II errors (false negatives). In general, increasing the true positive rate of a binary classifier will tend to increase its false positive rate. The receiver operating characteristic (ROC) curve of a binary classifier measures the cost of increasing the true positive rate, in terms of accepting higher false positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Añadir el directorio raíz del proyecto al PYTHONPATH\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import quantstats as qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de los labels sobre los datos originales. Si esto estuviese bien hecho, se habria hecho en el apartado 2. Pero como en ese momento no tenía ni puta idea, pues lo hago ahora con los datos completos. ya los uniré después.\n",
    "\n",
    "En este notebook cojo los datos originales (sin separar en in-sample y out-of-sample) y aplico 'three barrier method' sobre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar datos\n",
    "#SPY\n",
    "spy_data = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\00_api_data\\SPY_all.parquet')\n",
    "spy_dollar_imb = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\01_imbalance_bars\\SPY_dollar_imbalance.parquet')\n",
    "spy_volume_imb = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\01_imbalance_bars\\SPY_volume_imbalance.parquet')\n",
    "\n",
    "\n",
    "#BTC\n",
    "btc_data = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\00_api_data\\BTC_all.parquet')\n",
    "btc_dollar_imb = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\01_imbalance_bars\\BTC_dollar_imbalance.parquet')\n",
    "btc_volume_imb = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\01_imbalance_bars\\BTC_volume_imbalance.parquet')\n",
    "\n",
    "# paso todas las columnas date a formato datetime, y las convierto en índice.\n",
    "# Convertir la columna 'date' a datetime para todos los DataFrames\n",
    "spy_data['date'] = pd.to_datetime(spy_data['date'])\n",
    "spy_dollar_imb['date'] = pd.to_datetime(spy_dollar_imb['date'])\n",
    "spy_volume_imb['date'] = pd.to_datetime(spy_volume_imb['date'])\n",
    "\n",
    "btc_data['date'] = pd.to_datetime(btc_data['date'])\n",
    "btc_dollar_imb['date'] = pd.to_datetime(btc_dollar_imb['date'])\n",
    "btc_volume_imb['date'] = pd.to_datetime(btc_volume_imb['date'])\n",
    "\n",
    "# Establecer la columna 'date' como índice para todos los DataFrames\n",
    "spy_data.set_index('date', inplace=True)\n",
    "spy_dollar_imb.set_index('date', inplace=True)\n",
    "spy_volume_imb.set_index('date', inplace=True)\n",
    "\n",
    "btc_data.set_index('date', inplace=True)\n",
    "btc_dollar_imb.set_index('date', inplace=True)\n",
    "btc_volume_imb.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three barrier method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de ayuda\n",
    "\n",
    "funciones basadas en: https://towardsdatascience.com/financial-machine-learning-part-1-labels-7eeed050f32e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para calcular la volatilidad. El libro usa la diaría. aquí usaremos la horaria.\n",
    "def calculate_rolling_volatility(prices: pd.Series, span: int = 100, time_delta: pd.Timedelta = pd.Timedelta(hours=1)) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling volatility of a time series of prices.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices : pd.Series\n",
    "        A pandas Series representing the price data, indexed by time.\n",
    "    span : int, optional\n",
    "        The span for the exponential weighted moving average, by default 100.\n",
    "    time_delta : pd.Timedelta, optional\n",
    "        The time difference used to compute returns, by default 1 hour.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series containing the rolling volatility of the price series.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Calcular los retornos de la forma p[t]/p[t-1] - 1\n",
    "    # 1.1 Encontrar los timestamps de los valores p[t-1]\n",
    "    previous_indices = prices.index.searchsorted(prices.index - time_delta)\n",
    "    previous_indices = previous_indices[previous_indices > 0]\n",
    "\n",
    "    # 1.2 Alinear los timestamps de p[t-1] con los timestamps de p[t]\n",
    "    aligned_indices = pd.Series(prices.index[previous_indices-1], index=prices.index[prices.shape[0] - previous_indices.shape[0]:])\n",
    "\n",
    "    # 1.3 Obtener valores por timestamps, y luego calcular los retornos\n",
    "    returns = prices.loc[aligned_indices.index] / prices.loc[aligned_indices.values].values - 1\n",
    "\n",
    "    # 2. Estimar la desviación estándar móvil (volatilidad) usando media ponderada exponencialmente\n",
    "    rolling_volatility = returns.ewm(span=span).std()\n",
    "\n",
    "    return rolling_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horizons(prices: pd.Series, time_delta: pd.Timedelta = pd.Timedelta(minutes=60)) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the future time horizons for a given time series of prices.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices : pd.Series\n",
    "        A pandas Series representing the price data, indexed by time.\n",
    "    time_delta : pd.Timedelta, optional\n",
    "        The time difference used to calculate future time horizons, by default 60 minutes.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series containing the future time horizons, indexed by the original timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Buscar los índices de los timestamps desplazados hacia adelante por time_delta\n",
    "    future_indices = prices.index.searchsorted(prices.index + time_delta)\n",
    "    \n",
    "    # 2. Filtrar los índices que están dentro del rango del DataFrame\n",
    "    future_indices = future_indices[future_indices < prices.shape[0]]\n",
    "    \n",
    "    # 3. Obtener los timestamps correspondientes a los índices futuros\n",
    "    future_times = prices.index[future_indices]\n",
    "    \n",
    "    # 4. Crear una Serie con los timestamps futuros, indexada por los timestamps originales\n",
    "    time_horizons = pd.Series(future_times, index=prices.index[:future_times.shape[0]])\n",
    "\n",
    "    return time_horizons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_touches(prices: pd.Series, events: pd.DataFrame, factors: list = [1, 1]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the earliest stop loss and take profit times for given events.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices : pd.Series\n",
    "        Series with price data indexed by time.\n",
    "    events : pd.DataFrame\n",
    "        DataFrame with the following columns:\n",
    "            - t1: Timestamp of the next horizon.\n",
    "            - threshold: Unit height of the top and bottom barriers.\n",
    "            - side: The direction (side) of each bet.\n",
    "    factors : list, optional\n",
    "        Multipliers for the threshold to set the height of the top and bottom barriers.\n",
    "        Default is [1, 1], meaning the barriers are at 1x the threshold.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns 'stop_loss' and 'take_profit' indicating the earliest times\n",
    "        at which the stop loss and take profit levels are touched for each event.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear una copia del DataFrame 'events' con la columna 't1'\n",
    "    touch_times = events[['t1']].copy(deep=True)\n",
    "\n",
    "    # Calcular el umbral superior (barrera superior)\n",
    "    if factors[0] > 0:\n",
    "        upper_threshold = factors[0] * events['threshold']\n",
    "    else:\n",
    "        upper_threshold = pd.Series(index=events.index)  # sin umbral superior\n",
    "\n",
    "    # Calcular el umbral inferior (barrera inferior)\n",
    "    if factors[1] > 0:\n",
    "        lower_threshold = -factors[1] * events['threshold']\n",
    "    else:\n",
    "        lower_threshold = pd.Series(index=events.index)  # sin umbral inferior\n",
    "\n",
    "    # Iterar sobre cada evento para calcular el stop loss y take profit\n",
    "    for event_index, horizon_time in events['t1'].items():\n",
    "        price_path = prices[event_index:horizon_time]  # Precios en el camino\n",
    "        returns_path = (price_path / prices[event_index] - 1) * events.loc[event_index, 'side']  # Retornos en el camino\n",
    "        touch_times.loc[event_index, 'stop_loss'] = returns_path[returns_path < lower_threshold[event_index]].index.min()  # Primer stop loss\n",
    "        touch_times.loc[event_index, 'take_profit'] = returns_path[returns_path > upper_threshold[event_index]].index.min()  # Primer take profit\n",
    "\n",
    "    return touch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_horizons(prices, delta=pd.Timedelta(minutes=60)):\n",
    "#     t1 = prices.index.searchsorted(prices.index + delta)\n",
    "#     t1 = t1[t1 < prices.shape[0]]\n",
    "#     t1 = prices.index[t1]\n",
    "#     t1 = pd.Series(t1, index=prices.index[:t1.shape[0]])\n",
    "#     return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_touches(prices: pd.Series, events: pd.DataFrame, factors=[1, 1]) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculate the earliest stop loss and take profit for given events.\n",
    "    \n",
    "#     Parameters:\n",
    "#     prices : pd.Series\n",
    "#         Series with price data.\n",
    "#     events : pd.DataFrame\n",
    "#         DataFrame with columns:\n",
    "#             - t1: Timestamp of the next horizon\n",
    "#             - threshold: Unit height of top and bottom barriers\n",
    "#             - side: The side of each bet\n",
    "#     factors : list\n",
    "#         Multipliers of the threshold to set the height of top/bottom barriers.\n",
    "        \n",
    "#     Returns:\n",
    "#     pd.DataFrame\n",
    "#         DataFrame with columns 'stop_loss' and 'take_profit' for each event.\n",
    "#     \"\"\"\n",
    "#     out = events[['t1']].copy(deep=True)\n",
    "    \n",
    "#     if factors[0] > 0:\n",
    "#         thresh_uppr = factors[0] * events['threshold']\n",
    "#     else:\n",
    "#         thresh_uppr = pd.Series(index=events.index)  # no upper threshold\n",
    "    \n",
    "#     if factors[1] > 0:\n",
    "#         thresh_lwr = -factors[1] * events['threshold']\n",
    "#     else:\n",
    "#         thresh_lwr = pd.Series(index=events.index)  # no lower threshold\n",
    "    \n",
    "#     for loc, t1 in events['t1'].items():  # Cambiado a 'items' en lugar de 'iteritems'\n",
    "#         df0 = prices[loc:t1]  # path prices\n",
    "#         df0 = (df0 / prices[loc] - 1) * events.loc[loc, 'side']  # path returns\n",
    "#         out.loc[loc, 'stop_loss'] = df0[df0 < thresh_lwr[loc]].index.min()  # earliest stop loss\n",
    "#         out.loc[loc, 'take_profit'] = df0[df0 > thresh_uppr[loc]].index.min()  # earliest take profit\n",
    "    \n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(touches: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign labels to events based on the first touch of stop loss or take profit levels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    touches : pd.DataFrame\n",
    "        DataFrame containing the earliest stop loss and take profit times for each event.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with an additional 'label' column indicating the outcome:\n",
    "        - 1 for take profit.\n",
    "        - -1 for stop loss.\n",
    "        - 0 if neither was touched.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = touches.copy(deep=True)\n",
    "\n",
    "    # Calcular el primer nivel tocado (stop loss o take profit) ignorando valores NaN\n",
    "    first_touch = touches[['stop_loss', 'take_profit']].min(axis=1)\n",
    "\n",
    "    # Asignar etiquetas según el primer nivel tocado\n",
    "    for event_index, touch_time in first_touch.items():\n",
    "        if pd.isnull(touch_time):\n",
    "            labels.loc[event_index, 'label'] = 0  # No se tocó ningún nivel\n",
    "        elif touch_time == touches.loc[event_index, 'stop_loss']:\n",
    "            labels.loc[event_index, 'label'] = -1  # Se tocó el stop loss\n",
    "        else:\n",
    "            labels.loc[event_index, 'label'] = 1  # Se tocó el take profit\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def get_labels(touches: pd.DataFrame) -> pd.DataFrame:\n",
    "#     out = touches.copy(deep=True)\n",
    "#     # pandas df.min() ignores NaN values\n",
    "#     first_touch = touches[['stop_loss', 'take_profit']].min(axis=1)\n",
    "    \n",
    "#     for loc, t in first_touch.items():\n",
    "#         if pd.isnull(t):\n",
    "#             out.loc[loc, 'label'] = 0\n",
    "#         elif t == touches.loc[loc, 'stop_loss']:\n",
    "#             out.loc[loc, 'label'] = -1\n",
    "#         else:\n",
    "#             out.loc[loc, 'label'] = 1\n",
    "            \n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_ohlc_data(data_ohlc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process OHLC data to generate event labels for trading strategies.\n",
    "\n",
    "    This function calculates the volatility threshold, the time horizons, and assigns\n",
    "    labels for stop loss and take profit events.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    spy_data : pd.DataFrame\n",
    "        A DataFrame containing OHLC data with at least a 'close' price column.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the original data with additional columns:\n",
    "        - 'threshold': Volatility threshold calculated from the close prices.\n",
    "        - 't1': Time horizons for each event.\n",
    "        - 'label': Event labels (-1 for stop loss, 1 for take profit, 0 for neither).\n",
    "    \"\"\"\n",
    "\n",
    "    # Asignar umbral de volatilidad calculado a partir de los precios de cierre\n",
    "    data_ohlc = data_ohlc.assign(threshold=calculate_rolling_volatility(spy_data.close)).dropna()\n",
    "\n",
    "    # Asignar horizontes temporales calculados\n",
    "    data_ohlc = data_ohlc.assign(t1=get_horizons(data_ohlc)).dropna()\n",
    "\n",
    "    # Crear DataFrame de eventos con las columnas 't1' y 'threshold'\n",
    "    events = data_ohlc[['t1', 'threshold']]\n",
    "\n",
    "    # Asignar la columna 'side' con valor 1 para indicar posiciones largas únicamente\n",
    "    events = events.assign(side=pd.Series(1., index=events.index))\n",
    "\n",
    "    # Calcular los niveles de stop loss y take profit\n",
    "    touches = get_touches(data_ohlc.close, events, factors=[1, 1])\n",
    "\n",
    "    # Asignar etiquetas basadas en los niveles de stop loss y take profit\n",
    "    touches = get_labels(touches)\n",
    "\n",
    "    # Asignar las etiquetas finales al DataFrame original\n",
    "    data_ohlc = data_ohlc.assign(label=touches.label)\n",
    "\n",
    "    return data_ohlc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de labeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spy = process_ohlc_data(spy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spy.to_parquet('spy_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_volume_imb = process_ohlc_data(spy_volume_imb)\n",
    "spy_volume_imb.to_parquet('spy_volume_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_dollar_imb = process_ohlc_data(spy_dollar_imb)\n",
    "spy_dollar_imb.to_parquet('spy_dollar_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_data = process_ohlc_data(btc_data)\n",
    "btc_data.to_parquet('btc_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_volume_imb = process_ohlc_data(btc_volume_imb)\n",
    "btc_volume_imb.to_parquet('btc_volume_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_dollar_imb = process_ohlc_data(btc_dollar_imb)\n",
    "btc_dollar_imb.to_parquet('btc_dollar_with_labels.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices de confusión\n",
    "\n",
    "Una vez tengo los labeles, puedo obtener las matrices de confusion de los datos de los datos predichos por el algoritmo.\n",
    "\n",
    "El meta labeling es una clasificación binaria. Osea que los labeles {-1,1} son aciertos, y los {0} errores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_ORIGINAL_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\BTC_original_returns.parquet')\n",
    "BTC_VOLUME_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\BTC_volume_returns.parquet')\n",
    "BTC_DOLLAR_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\BTC_dollar_returns.parquet')\n",
    "\n",
    "SPY_ORIGINAL_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\SPY_original_returns.parquet')\n",
    "SPY_VOLUME_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\SPY_volume_returns.parquet')\n",
    "SPY_DOLLAR_RET = pd.read_parquet(r'C:\\Users\\adelapuente\\Desktop\\math_tfm\\04_validation\\SPY_dollar_returns.parquet')\n",
    "\n",
    "\n",
    "SPY_ORIGINAL_RET.set_index('date', inplace=True)\n",
    "SPY_VOLUME_RET.set_index('date', inplace=True)\n",
    "SPY_DOLLAR_RET.set_index('date', inplace=True)\n",
    "\n",
    "BTC_ORIGINAL_RET.set_index('date', inplace=True)\n",
    "BTC_VOLUME_RET.set_index('date', inplace=True)\n",
    "BTC_DOLLAR_RET.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPY Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35     17465\n",
      "           1       0.65      0.65      0.65     31804\n",
      "\n",
      "    accuracy                           0.54     49269\n",
      "   macro avg       0.50      0.50      0.50     49269\n",
      "weighted avg       0.54      0.54      0.54     49269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spy = pd.read_parquet('spy_with_labels.parquet')\n",
    "spy_original_union = SPY_ORIGINAL_RET.merge(new_spy['label'], left_index=True, right_index=True)\n",
    "\n",
    "spy_original_labels = spy_original_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "spy_original_ppo_predictions = spy_original_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "spy_original_union_cm = classification_report(spy_original_labels, spy_original_ppo_predictions)\n",
    "print(spy_original_union_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPY Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.37      0.38      1304\n",
      "           1       0.63      0.64      0.63      2149\n",
      "\n",
      "    accuracy                           0.54      3453\n",
      "   macro avg       0.50      0.50      0.50      3453\n",
      "weighted avg       0.53      0.54      0.54      3453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spy_volume_imb = pd.read_parquet('spy_volume_with_labels.parquet')\n",
    "spy_volume_union = SPY_VOLUME_RET.merge(spy_volume_imb['label'], left_index=True, right_index=True)\n",
    "\n",
    "spy_volume_labels = spy_volume_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "spy_volume_ppo_predictions = spy_volume_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "spy_volume_union_cm = classification_report(spy_volume_labels, spy_volume_ppo_predictions)\n",
    "print(spy_volume_union_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPY Dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.37      0.38      3161\n",
      "           1       0.63      0.65      0.64      5195\n",
      "\n",
      "    accuracy                           0.55      8356\n",
      "   macro avg       0.51      0.51      0.51      8356\n",
      "weighted avg       0.54      0.55      0.54      8356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spy_dollar_imb = pd.read_parquet('spy_dollar_with_labels.parquet')\n",
    "spy_dollar_union = SPY_DOLLAR_RET.merge(spy_dollar_imb['label'], left_index=True, right_index=True)\n",
    "\n",
    "spy_dollar_labels = spy_dollar_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "spy_dollar_ppo_predictions = spy_dollar_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "spy_dollar_union_cm = classification_report(spy_dollar_labels, spy_dollar_ppo_predictions)\n",
    "print(spy_dollar_union_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.18      0.04       903\n",
      "           1       0.98      0.86      0.92     49867\n",
      "\n",
      "    accuracy                           0.85     50770\n",
      "   macro avg       0.50      0.52      0.48     50770\n",
      "weighted avg       0.97      0.85      0.90     50770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_data = pd.read_parquet('btc_with_labels.parquet')\n",
    "btc_original_union = BTC_ORIGINAL_RET.merge(btc_data['label'], left_index=True, right_index=True)\n",
    "\n",
    "btc_original_labels = btc_original_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "btc_original_ppo_predictions = btc_original_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "btc_original_union_cm = classification_report(btc_original_labels, btc_original_ppo_predictions)\n",
    "print(btc_original_union_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.60      0.04       128\n",
      "           1       0.98      0.39      0.56      5961\n",
      "\n",
      "    accuracy                           0.40      6089\n",
      "   macro avg       0.50      0.50      0.30      6089\n",
      "weighted avg       0.96      0.40      0.55      6089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_volume_imb = pd.read_parquet('btc_volume_with_labels.parquet')\n",
    "btc_volume_union = BTC_VOLUME_RET.merge(btc_volume_imb['label'], left_index=True, right_index=True)\n",
    "\n",
    "btc_volume_labels = btc_volume_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "btc_volume_ppo_predictions = btc_volume_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "btc_volume_union_cm = classification_report(btc_volume_labels, btc_volume_ppo_predictions)\n",
    "print(btc_volume_union_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC Dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.31      0.03       850\n",
      "           1       0.98      0.64      0.78     45678\n",
      "\n",
      "    accuracy                           0.64     46528\n",
      "   macro avg       0.50      0.48      0.40     46528\n",
      "weighted avg       0.96      0.64      0.76     46528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_dollar_imb = pd.read_parquet('btc_dollar_with_labels.parquet')\n",
    "btc_dollar_union = BTC_DOLLAR_RET.merge(btc_dollar_imb['label'], left_index=True, right_index=True)\n",
    "\n",
    "btc_dollar_labels = btc_dollar_union['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "btc_dollar_ppo_predictions = btc_dollar_union['predicted_actions'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "btc_dollar_union_cm = classification_report(btc_dollar_labels, btc_dollar_ppo_predictions)\n",
    "print(btc_dollar_union_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
